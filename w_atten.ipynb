{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Input, Dropout, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./data.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = df[['text', 'target']]\n",
    "\n",
    "# Map target to two classes: negative, positive\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Sample 66,666 instances from each class to get a total of 200,000 samples\n",
    "neg_df = df[df['target'] == 0].sample(n=66666, random_state=42)\n",
    "pos_df = df[df['target'] == 1].sample(n=66666, random_state=42)\n",
    "# Combine the sampled data\n",
    "df_sampled = pd.concat([neg_df, pos_df])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df_sampled\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtag symbol\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = re.sub(r'https?://\\S+', '', text)  # Remove the hyperlink\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove special characters\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# One-hot encode the target\n",
    "Y = df['target'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file_path, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Create an embedding matrix\n",
    "def create_embedding_matrix(word_index, embeddings_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to GloVe file\n",
    "glove_file_path = 'glove.twitter.27B.100d.txt'\n",
    "embedding_dim = 100  # Depending on the GloVe file used\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = load_glove_embeddings(glove_file_path, embedding_dim)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embeddings_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">356</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │           \u001b[38;5;34m300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_2 (\u001b[38;5;33mAttention\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m356\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,409</span> (919.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m235,409\u001b[0m (919.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,109</span> (918.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m235,109\u001b[0m (918.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> (1.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m300\u001b[0m (1.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Assuming `tokenizer` and `embedding_matrix` are defined\n",
    "tokenizer = {'word_index': {'word1': 1, 'word2': 2}}  # Example tokenizer\n",
    "embedding_matrix = np.random.rand(len(tokenizer['word_index']) + 1, 100)  # Example embedding matrix\n",
    "\n",
    "# Model definition\n",
    "input = Input(shape=(100,))\n",
    "x = Embedding(input_dim=len(tokenizer['word_index']) + 1, output_dim=100, weights=[embedding_matrix], trainable=False)(input)\n",
    "x = SpatialDropout1D(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, activation='relu', dropout=0.5, recurrent_dropout=0.5, return_sequences=True))(x)\n",
    "x = Attention()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1/embedding_2_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Prakhar\\AppData\\Local\\Temp\\ipykernel_19072\\3408593971.py\", line 2, in <module>\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 51, in train_step\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 175, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 556, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 4875, in take\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1951, in take\n\nindices[5,77] = 22 is not in [0, 3)\n\t [[{{node functional_1/embedding_2_1/GatherV2}}]] [Op:__inference_one_step_on_iterator_5650]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Final training on the full dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test), callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_1/embedding_2_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Prakhar\\AppData\\Local\\Temp\\ipykernel_19072\\3408593971.py\", line 2, in <module>\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 51, in train_step\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 175, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 556, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 4875, in take\n\n  File \"c:\\Users\\Prakhar\\anaconda3\\envs\\lat\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1951, in take\n\nindices[5,77] = 22 is not in [0, 3)\n\t [[{{node functional_1/embedding_2_1/GatherV2}}]] [Op:__inference_one_step_on_iterator_5650]"
     ]
    }
   ],
   "source": [
    "# Final training on the full dataset\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=10, validation_data=(X_test, Y_test), callbacks=[early_stopping], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Depending on the GloVe file used\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Load GloVe embeddings\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m embeddings_index \u001b[38;5;241m=\u001b[39m load_glove_embeddings(glove_file_path, embedding_dim)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Create the embedding matrix\u001b[39;00m\n\u001b[0;32m     97\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m create_embedding_matrix(tokenizer\u001b[38;5;241m.\u001b[39mword_index, embeddings_index, embedding_dim)\n",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m, in \u001b[0;36mload_glove_embeddings\u001b[1;34m(glove_file_path, embedding_dim)\u001b[0m\n\u001b[0;32m     71\u001b[0m embeddings_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(glove_file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     74\u001b[0m         values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     75\u001b[0m         word \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming your Attention layer is defined as before\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = df[['text', 'target']]\n",
    "\n",
    "# Map target to two classes: negative, positive\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Sample 66,666 instances from each class to get a total of 200,000 samples\n",
    "neg_df = df[df['target'] == 0].sample(n=66666, random_state=42)\n",
    "pos_df = df[df['target'] == 1].sample(n=66666, random_state=42)\n",
    "# Combine the sampled data\n",
    "df_sampled = pd.concat([neg_df, pos_df])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df_sampled\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtag symbol\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = re.sub(r'https?://\\S+', '', text)  # Remove the hyperlink\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove special characters\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# One-hot encode the target\n",
    "Y = df['target'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file_path, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Create an embedding matrix\n",
    "def create_embedding_matrix(word_index, embeddings_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to GloVe file\n",
    "glove_file_path = 'glove.twitter.27B.100d.txt'\n",
    "embedding_dim = 100  # Depending on the GloVe file used\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = load_glove_embeddings(glove_file_path, embedding_dim)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embeddings_index, embedding_dim)\n",
    "\n",
    "# Build the model\n",
    "input = Input(shape=(100,))\n",
    "x = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)(input)\n",
    "x = SpatialDropout1D(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, activation='relu', dropout=0.5, recurrent_dropout=0.5, return_sequences=True))(x)\n",
    "x = Attention()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_data=(X_test, Y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Example of classification report\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_classes = (Y_pred > 0.5).astype(\"int32\")\n",
    "print(classification_report(Y_test, Y_pred_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
