{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Verify TensorFlow is using the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./data.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = df[['text', 'target']]\n",
    "\n",
    "# Map target to two classes: negative, positive\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})\n",
    "\n",
    "# Sample 66,666 instances from each class to get a total of 200,000 samples\n",
    "neg_df = df[df['target'] == 0].sample(n=66666, random_state=42)\n",
    "pos_df = df[df['target'] == 1].sample(n=66666, random_state=42)\n",
    "# Combine the sampled data\n",
    "df_sampled = pd.concat([neg_df, pos_df])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtag symbol\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # Remove RT\n",
    "    text = re.sub(r'https?://\\S+', '', text)  # Remove the hyper link\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove special characters\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# One-hot encode the target\n",
    "Y = df['target'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained embeddings (commented, assuming embeddings are not provided)\n",
    "# embedding_matrix = ...  # Load pre-trained embeddings\n",
    "# embedding_layer = Embedding(input_dim=5000, output_dim=128, weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128))  # Use embedding_layer if using pre-trained embeddings\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Bidirectional(LSTM(128, activation='relu', dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, activation='relu', dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(0.01))))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0001, decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation (example)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_k, X_val_k = X[train_index], X[val_index]\n",
    "    Y_train_k, Y_val_k = Y[train_index], Y[val_index]\n",
    "    history = model.fit(X_train_k, Y_train_k, epochs=8, batch_size=10, validation_data=(X_val_k, Y_val_k), callbacks=[early_stopping], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final training on the full dataset\n",
    "history = model.fit(X_train, Y_train, epochs=8, batch_size=10, validation_data=(X_test, Y_test), callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "# Feature engineering (commented, as an example)\n",
    "# additional_features = ...\n",
    "# X_combined = np.concatenate([X, additional_features], axis=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
